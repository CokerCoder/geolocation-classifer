{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocation of tweets classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfe/miniconda3/envs/iml-p3/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## for data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train and dev data\n",
    "df_train = pd.read_csv('data/train_full.csv')\n",
    "df_train = df_train[['tweet', 'region']]\n",
    "\n",
    "df_dev = pd.read_csv('data/dev_full.csv')\n",
    "df_dev = df_dev[['tweet', 'region']]\n",
    "\n",
    "df_train.index = range(df_train.shape[0])\n",
    "df_dev.index = range(df_dev.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133795, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12287</th>\n",
       "      <td>@USER_da4dba38 dm'd? Wth? Lol</td>\n",
       "      <td>MIDWEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35407</th>\n",
       "      <td>@USER_8752179b @USER_8c22f85f And you know its...</td>\n",
       "      <td>MIDWEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76810</th>\n",
       "      <td>@USER_59fef649 ari wants to go venus</td>\n",
       "      <td>NORTHEAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73041</th>\n",
       "      <td>@USER_2f0ad3d1 trevor ariza is better for the ...</td>\n",
       "      <td>NORTHEAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103686</th>\n",
       "      <td>.@USER_59b459e0 1 of CA's biggest expenses is ...</td>\n",
       "      <td>WEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16225</th>\n",
       "      <td>@USER_295ab1b6 is that screen gonna be a touch...</td>\n",
       "      <td>SOUTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79229</th>\n",
       "      <td>\"I wish there was a site for Jewish teens so I...</td>\n",
       "      <td>NORTHEAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119033</th>\n",
       "      <td>RT @USER_72cdae8d: @USER_25d0ba74 and blair un...</td>\n",
       "      <td>WEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7456</th>\n",
       "      <td>@USER_77a8f85d I'ma b lonely wit 2 dogs n $$$,...</td>\n",
       "      <td>SOUTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81029</th>\n",
       "      <td>I love being with him even when we argued &amp;&amp; h...</td>\n",
       "      <td>WEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet     region\n",
       "12287                       @USER_da4dba38 dm'd? Wth? Lol    MIDWEST\n",
       "35407   @USER_8752179b @USER_8c22f85f And you know its...    MIDWEST\n",
       "76810                @USER_59fef649 ari wants to go venus  NORTHEAST\n",
       "73041   @USER_2f0ad3d1 trevor ariza is better for the ...  NORTHEAST\n",
       "103686  .@USER_59b459e0 1 of CA's biggest expenses is ...       WEST\n",
       "16225   @USER_295ab1b6 is that screen gonna be a touch...      SOUTH\n",
       "79229   \"I wish there was a site for Jewish teens so I...  NORTHEAST\n",
       "119033  RT @USER_72cdae8d: @USER_25d0ba74 and blair un...       WEST\n",
       "7456    @USER_77a8f85d I'ma b lonely wit 2 dogs n $$$,...      SOUTH\n",
       "81029   I love being with him even when we argued && h...       WEST"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"count\", fontsize=12)\n",
    "df_train[\"region\"].reset_index().groupby(\"region\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess a string.\n",
    ":parameter\n",
    "    :param text: string - name of column containing text\n",
    "    :param lst_stopwords: list - list of stopwords to remove\n",
    "    :param flg_stemm: bool - whether stemming is to be applied\n",
    "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    ":return\n",
    "    cleaned text\n",
    "'''\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    \n",
    "    # remove @mentions (@USER)\n",
    "#     text = re.sub(r'\\B@USER\\w+', '', text)\n",
    "    \n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = nltk.word_tokenize(text)    \n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yunfe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yunfe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yunfe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lst_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>region</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22148</th>\n",
       "      <td>A bunch ah assholeness... Leh me going in my b...</td>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>bunch ah assholeness leh going bed wh ppl real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22186</th>\n",
       "      <td>#BBM 3079DAE9 Get At Me!</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>bbm 3079dae9 get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115048</th>\n",
       "      <td>Fuck yea! RT @USER_e613e67f: LOVE IT! RT @USER...</td>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>fuck yea rt user_e613e67f love rt user_6e2949f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80056</th>\n",
       "      <td>#shoutout to my #TTfam @USER_d7b3fb95 -- this ...</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>shoutout ttfam user_d7b3fb95 must follow imjus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74179</th>\n",
       "      <td>PEOPLE WHO TYPE IN ALL CAPS JUST WANT ATTENTION</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>people type cap want attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93173</th>\n",
       "      <td>@USER_71f8c8df thanks twin</td>\n",
       "      <td>MIDWEST</td>\n",
       "      <td>user_71f8c8df thanks twin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16154</th>\n",
       "      <td>@USER_79491af4 yess ma'am</td>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>user_79491af4 yes maam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89129</th>\n",
       "      <td>3140ACFF (No DUDES!)</td>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>3140acff dude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12960</th>\n",
       "      <td>@USER_91550f7c yuuup! Catch me if ya can.. ::V...</td>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>user_91550f7c yuuup catch ya vrooooooooooooooom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44503</th>\n",
       "      <td>Goodmorning ppl,good to see everyone haven I g...</td>\n",
       "      <td>MIDWEST</td>\n",
       "      <td>goodmorning pplgood see everyone good morningh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet     region  \\\n",
       "22148   A bunch ah assholeness... Leh me going in my b...  NORTHEAST   \n",
       "22186                            #BBM 3079DAE9 Get At Me!      SOUTH   \n",
       "115048  Fuck yea! RT @USER_e613e67f: LOVE IT! RT @USER...  NORTHEAST   \n",
       "80056   #shoutout to my #TTfam @USER_d7b3fb95 -- this ...      SOUTH   \n",
       "74179     PEOPLE WHO TYPE IN ALL CAPS JUST WANT ATTENTION      SOUTH   \n",
       "93173                          @USER_71f8c8df thanks twin    MIDWEST   \n",
       "16154                           @USER_79491af4 yess ma'am  NORTHEAST   \n",
       "89129                                3140ACFF (No DUDES!)  NORTHEAST   \n",
       "12960   @USER_91550f7c yuuup! Catch me if ya can.. ::V...  NORTHEAST   \n",
       "44503   Goodmorning ppl,good to see everyone haven I g...    MIDWEST   \n",
       "\n",
       "                                              tweet_clean  \n",
       "22148   bunch ah assholeness leh going bed wh ppl real...  \n",
       "22186                                    bbm 3079dae9 get  \n",
       "115048  fuck yea rt user_e613e67f love rt user_6e2949f...  \n",
       "80056   shoutout ttfam user_d7b3fb95 must follow imjus...  \n",
       "74179                      people type cap want attention  \n",
       "93173                           user_71f8c8df thanks twin  \n",
       "16154                              user_79491af4 yes maam  \n",
       "89129                                       3140acff dude  \n",
       "12960     user_91550f7c yuuup catch ya vrooooooooooooooom  \n",
       "44503   goodmorning pplgood see everyone good morningh...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"tweet_clean\"] = df_train[\"tweet\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=lst_stopwords))\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev[\"tweet_clean\"] = df_dev[\"tweet\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.word2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = df_train[\"tweet_clean\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus_train = []\n",
    "for string in corpus_train:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus_train.append(lst_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['watching', 'lost'],\n",
       " ['user_89a3500b'],\n",
       " ['maneuver', 'put', 'team', 'hopefully', 'sooner', 'live', 'dream'],\n",
       " ['darko', 'eating', 'hamburger', 'locker', 'room', 'played', 'knicks', 'lol'],\n",
       " ['girl', 'pack', 'ya', 'bag', 'im', 'bout', 'take', 'ride']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_corpus_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detect bigrams and trigrams\n",
    "# bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "# bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "# trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "# trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the dev data as well\n",
    "corpus_dev = df_dev[\"tweet_clean\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus_dev = []\n",
    "for string in corpus_dev:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus_dev.append(lst_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.word2vec.Word2Vec(lst_corpus_train, vector_size=100, window=8, min_count=1, sg=1, epochs=20, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save(\"./models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index_to_key, model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06774512, -0.14816962,  0.17621005,  0.33268836,  0.05907883,\n",
       "       -0.2656425 , -0.10598019,  0.21918313, -0.17052585,  0.23545113,\n",
       "        0.20920493, -0.2029693 , -0.17596179,  0.18369879, -0.0277874 ,\n",
       "       -0.4078686 , -0.26955852, -0.5336092 , -0.00820513, -0.52968144,\n",
       "        0.04153088, -0.18102331,  0.1749128 , -0.1204199 , -0.0520412 ,\n",
       "        0.0574724 , -0.14681876,  0.25412348, -0.2804768 ,  0.00608826,\n",
       "        0.16467646,  0.11705982,  0.2215864 , -0.37483382, -0.08821878,\n",
       "        0.02303753, -0.0032479 , -0.04983553, -0.21186511, -0.48913062,\n",
       "       -0.12018241, -0.06363413, -0.05865871,  0.09006874, -0.12049423,\n",
       "       -0.11522233,  0.07892998, -0.01900528,  0.51069576,  0.30857173,\n",
       "        0.01094478,  0.03949932, -0.16969006, -0.01494583,  0.04894175,\n",
       "        0.31498817,  0.06585271, -0.26974118,  0.01435023, -0.0881516 ,\n",
       "       -0.06430107,  0.3303579 ,  0.00216886, -0.01905708, -0.2758078 ,\n",
       "        0.60310227,  0.17387281,  0.34696668, -0.1884546 ,  0.03943551,\n",
       "       -0.09580436,  0.06902882,  0.19028962, -0.15378684,  0.10522966,\n",
       "        0.24518731, -0.16571495,  0.00131248, -0.14383233, -0.08029936,\n",
       "       -0.21877918,  0.09790018, -0.10404528,  0.39163095,  0.03477583,\n",
       "       -0.04283436,  0.23773268,  0.14560434,  0.4439031 , -0.07378495,\n",
       "       -0.02816977, -0.15156417,  0.04126915, -0.14856012,  0.0720238 ,\n",
       "        0.43973437, -0.13496779,  0.10681808, -0.02439027, -0.03023829],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now every word is a word-vector\n",
    "w2v['donald']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 0.8730496764183044),\n",
       " ('lawnwit', 0.8397161960601807),\n",
       " ('user_c28d47f2', 0.830009937286377),\n",
       " ('combover', 0.8218833804130554),\n",
       " ('snorting', 0.8043967485427856),\n",
       " ('user_f8332890', 0.7982161045074463),\n",
       " ('user_8674f459', 0.7913151383399963),\n",
       " ('user_62fa06b6', 0.7910611629486084),\n",
       " ('peanutbutter', 0.7896377444267273),\n",
       " ('situps', 0.7884388566017151)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['donald'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['donald'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to embedded vector by averaging the word vectors\n",
    "def transform(word2vec, words_list):\n",
    "    return np.array([\n",
    "        np.mean([word2vec[word] for word in words if word in word2vec]\n",
    "                or [np.zeros(100)], axis=0)\n",
    "        for words in words_list\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors_train = transform(w2v, lst_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133795, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors_dev = transform(w2v, lst_corpus_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11475, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vectors_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = w2v_vectors_train\n",
    "y_train = df_train['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = w2v_vectors_dev\n",
    "y_dev = df_dev['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(x_train)==len(y_train))\n",
    "assert(len(x_dev)==len(y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_clf.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR \t\tAccuracy: 0.45\tMacro F1: 0.26\n"
     ]
    }
   ],
   "source": [
    "lr_acc = accuracy_score(y_dev, lr_predictions)\n",
    "lr_f1 = f1_score(y_dev, lr_predictions, average='macro')\n",
    "\n",
    "print(f\"LR \\t\\tAccuracy: {round(lr_acc, 2)}\\tMacro F1: {round(lr_f1, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf = ExtraTreesClassifier(n_estimators=100, random_state=0).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_predictions = et_clf.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTrees \t\tAccuracy: 0.43\tMacro F1: 0.25\n"
     ]
    }
   ],
   "source": [
    "et_acc = accuracy_score(y_dev, et_predictions)\n",
    "et_f1 = f1_score(y_dev, et_predictions, average='macro')\n",
    "\n",
    "print(f\"ExtraTrees \\t\\tAccuracy: {round(et_acc, 2)}\\tMacro F1: {round(et_f1, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.20474903\n",
      "Iteration 2, loss = 1.18622760\n",
      "Iteration 3, loss = 1.17965831\n",
      "Iteration 4, loss = 1.17475777\n",
      "Iteration 5, loss = 1.17064095\n",
      "Iteration 6, loss = 1.16658012\n",
      "Iteration 7, loss = 1.16350200\n",
      "Iteration 8, loss = 1.16034639\n",
      "Iteration 9, loss = 1.15729543\n",
      "Iteration 10, loss = 1.15471909\n",
      "Iteration 11, loss = 1.15200088\n",
      "Iteration 12, loss = 1.14941628\n",
      "Iteration 13, loss = 1.14743245\n",
      "Iteration 14, loss = 1.14579632\n",
      "Iteration 15, loss = 1.14338944\n",
      "Iteration 16, loss = 1.14198371\n",
      "Iteration 17, loss = 1.14001481\n",
      "Iteration 18, loss = 1.13852369\n",
      "Iteration 19, loss = 1.13673927\n",
      "Iteration 20, loss = 1.13571929\n",
      "Iteration 21, loss = 1.13414519\n",
      "Iteration 22, loss = 1.13319212\n",
      "Iteration 23, loss = 1.13192619\n",
      "Iteration 24, loss = 1.13095931\n",
      "Iteration 25, loss = 1.12919789\n",
      "Iteration 26, loss = 1.12855266\n",
      "Iteration 27, loss = 1.12752073\n",
      "Iteration 28, loss = 1.12641187\n",
      "Iteration 29, loss = 1.12566293\n",
      "Iteration 30, loss = 1.12497286\n",
      "Iteration 31, loss = 1.12371240\n",
      "Iteration 32, loss = 1.12272813\n",
      "Iteration 33, loss = 1.12155060\n",
      "Iteration 34, loss = 1.12133896\n",
      "Iteration 35, loss = 1.12069571\n",
      "Iteration 36, loss = 1.11942624\n",
      "Iteration 37, loss = 1.11891386\n",
      "Iteration 38, loss = 1.11819424\n",
      "Iteration 39, loss = 1.11763815\n",
      "Iteration 40, loss = 1.11679797\n",
      "Iteration 41, loss = 1.11620264\n",
      "Iteration 42, loss = 1.11545838\n",
      "Iteration 43, loss = 1.11526845\n",
      "Iteration 44, loss = 1.11454492\n",
      "Iteration 45, loss = 1.11365775\n",
      "Iteration 46, loss = 1.11287082\n",
      "Iteration 47, loss = 1.11324302\n",
      "Iteration 48, loss = 1.11279456\n",
      "Iteration 49, loss = 1.11217597\n",
      "Iteration 50, loss = 1.11101922\n",
      "Iteration 51, loss = 1.11100781\n",
      "Iteration 52, loss = 1.11061236\n",
      "Iteration 53, loss = 1.10998491\n",
      "Iteration 54, loss = 1.10995666\n",
      "Iteration 55, loss = 1.10957917\n",
      "Iteration 56, loss = 1.10818462\n",
      "Iteration 57, loss = 1.10861884\n",
      "Iteration 58, loss = 1.10796747\n",
      "Iteration 59, loss = 1.10785122\n",
      "Iteration 60, loss = 1.10723589\n",
      "Iteration 61, loss = 1.10674991\n",
      "Iteration 62, loss = 1.10678601\n",
      "Iteration 63, loss = 1.10600080\n",
      "Iteration 64, loss = 1.10578847\n",
      "Iteration 65, loss = 1.10552286\n",
      "Iteration 66, loss = 1.10536696\n",
      "Iteration 67, loss = 1.10510630\n",
      "Iteration 68, loss = 1.10437787\n",
      "Iteration 69, loss = 1.10430768\n",
      "Iteration 70, loss = 1.10315962\n",
      "Iteration 71, loss = 1.10386620\n",
      "Iteration 72, loss = 1.10318917\n",
      "Iteration 73, loss = 1.10252509\n",
      "Iteration 74, loss = 1.10210712\n",
      "Iteration 75, loss = 1.10234909\n",
      "Iteration 76, loss = 1.10224068\n",
      "Iteration 77, loss = 1.10183511\n",
      "Iteration 78, loss = 1.10097867\n",
      "Iteration 79, loss = 1.10161565\n",
      "Iteration 80, loss = 1.10079501\n",
      "Iteration 81, loss = 1.10093984\n",
      "Iteration 82, loss = 1.10010525\n",
      "Iteration 83, loss = 1.09956374\n",
      "Iteration 84, loss = 1.10002752\n",
      "Iteration 85, loss = 1.09961874\n",
      "Iteration 86, loss = 1.09922677\n",
      "Iteration 87, loss = 1.09935869\n",
      "Iteration 88, loss = 1.09900415\n",
      "Iteration 89, loss = 1.09894798\n",
      "Iteration 90, loss = 1.09832499\n",
      "Iteration 91, loss = 1.09830878\n",
      "Iteration 92, loss = 1.09874333\n",
      "Iteration 93, loss = 1.09781228\n",
      "Iteration 94, loss = 1.09846426\n",
      "Iteration 95, loss = 1.09751501\n",
      "Iteration 96, loss = 1.09739777\n",
      "Iteration 97, loss = 1.09670107\n",
      "Iteration 98, loss = 1.09661196\n",
      "Iteration 99, loss = 1.09666704\n",
      "Iteration 100, loss = 1.09674995\n",
      "Iteration 101, loss = 1.09608740\n",
      "Iteration 102, loss = 1.09670793\n",
      "Iteration 103, loss = 1.09623232\n",
      "Iteration 104, loss = 1.09576163\n",
      "Iteration 105, loss = 1.09608373\n",
      "Iteration 106, loss = 1.09533374\n",
      "Iteration 107, loss = 1.09541800\n",
      "Iteration 108, loss = 1.09537734\n",
      "Iteration 109, loss = 1.09491894\n",
      "Iteration 110, loss = 1.09502456\n",
      "Iteration 111, loss = 1.09448020\n",
      "Iteration 112, loss = 1.09434232\n",
      "Iteration 113, loss = 1.09419720\n",
      "Iteration 114, loss = 1.09420674\n",
      "Iteration 115, loss = 1.09373811\n",
      "Iteration 116, loss = 1.09361214\n",
      "Iteration 117, loss = 1.09340102\n",
      "Iteration 118, loss = 1.09382437\n",
      "Iteration 119, loss = 1.09319121\n",
      "Iteration 120, loss = 1.09364250\n",
      "Iteration 121, loss = 1.09297608\n",
      "Iteration 122, loss = 1.09260165\n",
      "Iteration 123, loss = 1.09248944\n",
      "Iteration 124, loss = 1.09265641\n",
      "Iteration 125, loss = 1.09220151\n",
      "Iteration 126, loss = 1.09248456\n",
      "Iteration 127, loss = 1.09227351\n",
      "Iteration 128, loss = 1.09201467\n",
      "Iteration 129, loss = 1.09235729\n",
      "Iteration 130, loss = 1.09178733\n",
      "Iteration 131, loss = 1.09161767\n",
      "Iteration 132, loss = 1.09161795\n",
      "Iteration 133, loss = 1.09147835\n",
      "Iteration 134, loss = 1.09130174\n",
      "Iteration 135, loss = 1.09134886\n",
      "Iteration 136, loss = 1.09072581\n",
      "Iteration 137, loss = 1.09120184\n",
      "Iteration 138, loss = 1.09098653\n",
      "Iteration 139, loss = 1.09012976\n",
      "Iteration 140, loss = 1.09055289\n",
      "Iteration 141, loss = 1.09044353\n",
      "Iteration 142, loss = 1.09046808\n",
      "Iteration 143, loss = 1.09030785\n",
      "Iteration 144, loss = 1.09014508\n",
      "Iteration 145, loss = 1.09006232\n",
      "Iteration 146, loss = 1.08973040\n",
      "Iteration 147, loss = 1.09032017\n",
      "Iteration 148, loss = 1.08920651\n",
      "Iteration 149, loss = 1.08956527\n",
      "Iteration 150, loss = 1.08914751\n",
      "Iteration 151, loss = 1.08973557\n",
      "Iteration 152, loss = 1.08938580\n",
      "Iteration 153, loss = 1.08897044\n",
      "Iteration 154, loss = 1.08865163\n",
      "Iteration 155, loss = 1.08932690\n",
      "Iteration 156, loss = 1.08915733\n",
      "Iteration 157, loss = 1.08879957\n",
      "Iteration 158, loss = 1.08842304\n",
      "Iteration 159, loss = 1.08863560\n",
      "Iteration 160, loss = 1.08814088\n",
      "Iteration 161, loss = 1.08786227\n",
      "Iteration 162, loss = 1.08828355\n",
      "Iteration 163, loss = 1.08800793\n",
      "Iteration 164, loss = 1.08780682\n",
      "Iteration 165, loss = 1.08787271\n",
      "Iteration 166, loss = 1.08757096\n",
      "Iteration 167, loss = 1.08799692\n",
      "Iteration 168, loss = 1.08751755\n",
      "Iteration 169, loss = 1.08741821\n",
      "Iteration 170, loss = 1.08748290\n",
      "Iteration 171, loss = 1.08744440\n",
      "Iteration 172, loss = 1.08710595\n",
      "Iteration 173, loss = 1.08717067\n",
      "Iteration 174, loss = 1.08669035\n",
      "Iteration 175, loss = 1.08662592\n",
      "Iteration 176, loss = 1.08673587\n",
      "Iteration 177, loss = 1.08693959\n",
      "Iteration 178, loss = 1.08694178\n",
      "Iteration 179, loss = 1.08629806\n",
      "Iteration 180, loss = 1.08599625\n",
      "Iteration 181, loss = 1.08629348\n",
      "Iteration 182, loss = 1.08625008\n",
      "Iteration 183, loss = 1.08606760\n",
      "Iteration 184, loss = 1.08555817\n",
      "Iteration 185, loss = 1.08622048\n",
      "Iteration 186, loss = 1.08582049\n",
      "Iteration 187, loss = 1.08564030\n",
      "Iteration 188, loss = 1.08555622\n",
      "Iteration 189, loss = 1.08560207\n",
      "Iteration 190, loss = 1.08534389\n",
      "Iteration 191, loss = 1.08540730\n",
      "Iteration 192, loss = 1.08520461\n",
      "Iteration 193, loss = 1.08531677\n",
      "Iteration 194, loss = 1.08529258\n",
      "Iteration 195, loss = 1.08510619\n",
      "Iteration 196, loss = 1.08552513\n",
      "Iteration 197, loss = 1.08513915\n",
      "Iteration 198, loss = 1.08522364\n",
      "Iteration 199, loss = 1.08500674\n",
      "Iteration 200, loss = 1.08467347\n",
      "Iteration 1, loss = 1.20236456\n",
      "Iteration 2, loss = 1.18489298\n",
      "Iteration 3, loss = 1.17821211\n",
      "Iteration 4, loss = 1.17334491\n",
      "Iteration 5, loss = 1.16908787\n",
      "Iteration 6, loss = 1.16492811\n",
      "Iteration 7, loss = 1.16169591\n",
      "Iteration 8, loss = 1.15830374\n",
      "Iteration 9, loss = 1.15523902\n",
      "Iteration 10, loss = 1.15251590\n",
      "Iteration 11, loss = 1.15052619\n",
      "Iteration 12, loss = 1.14815444\n",
      "Iteration 13, loss = 1.14590095\n",
      "Iteration 14, loss = 1.14396464\n",
      "Iteration 15, loss = 1.14201146\n",
      "Iteration 16, loss = 1.14065820\n",
      "Iteration 17, loss = 1.13932781\n",
      "Iteration 18, loss = 1.13745817\n",
      "Iteration 19, loss = 1.13601306\n",
      "Iteration 20, loss = 1.13469240\n",
      "Iteration 21, loss = 1.13278246\n",
      "Iteration 22, loss = 1.13218557\n",
      "Iteration 23, loss = 1.13051230\n",
      "Iteration 24, loss = 1.12967995\n",
      "Iteration 25, loss = 1.12849858\n",
      "Iteration 26, loss = 1.12735721\n",
      "Iteration 27, loss = 1.12651050\n",
      "Iteration 28, loss = 1.12538332\n",
      "Iteration 29, loss = 1.12490948\n",
      "Iteration 30, loss = 1.12322158\n",
      "Iteration 31, loss = 1.12300882\n",
      "Iteration 32, loss = 1.12165916\n",
      "Iteration 33, loss = 1.12145407\n",
      "Iteration 34, loss = 1.12057370\n",
      "Iteration 35, loss = 1.12016370\n",
      "Iteration 36, loss = 1.11928948\n",
      "Iteration 37, loss = 1.11847263\n",
      "Iteration 38, loss = 1.11767463\n",
      "Iteration 39, loss = 1.11707418\n",
      "Iteration 40, loss = 1.11577409\n",
      "Iteration 41, loss = 1.11585498\n",
      "Iteration 42, loss = 1.11511218\n",
      "Iteration 43, loss = 1.11489763\n",
      "Iteration 44, loss = 1.11389261\n",
      "Iteration 45, loss = 1.11286516\n",
      "Iteration 46, loss = 1.11236609\n",
      "Iteration 47, loss = 1.11211121\n",
      "Iteration 48, loss = 1.11225693\n",
      "Iteration 49, loss = 1.11160008\n",
      "Iteration 50, loss = 1.11074177\n",
      "Iteration 51, loss = 1.11001236\n",
      "Iteration 52, loss = 1.10995706\n",
      "Iteration 53, loss = 1.10949173\n",
      "Iteration 54, loss = 1.10896869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 1.10820509\n",
      "Iteration 56, loss = 1.10807089\n",
      "Iteration 57, loss = 1.10788806\n",
      "Iteration 58, loss = 1.10749958\n",
      "Iteration 59, loss = 1.10670247\n",
      "Iteration 60, loss = 1.10619447\n",
      "Iteration 61, loss = 1.10606675\n",
      "Iteration 62, loss = 1.10584148\n",
      "Iteration 63, loss = 1.10482582\n",
      "Iteration 64, loss = 1.10452497\n",
      "Iteration 65, loss = 1.10503086\n",
      "Iteration 66, loss = 1.10420955\n",
      "Iteration 67, loss = 1.10407935\n",
      "Iteration 68, loss = 1.10358420\n",
      "Iteration 69, loss = 1.10306565\n",
      "Iteration 70, loss = 1.10321563\n",
      "Iteration 71, loss = 1.10269065\n",
      "Iteration 72, loss = 1.10236084\n",
      "Iteration 73, loss = 1.10184244\n",
      "Iteration 74, loss = 1.10168232\n",
      "Iteration 75, loss = 1.10107675\n",
      "Iteration 76, loss = 1.10113448\n",
      "Iteration 77, loss = 1.10053066\n",
      "Iteration 78, loss = 1.10079512\n",
      "Iteration 79, loss = 1.09983875\n",
      "Iteration 80, loss = 1.09985463\n",
      "Iteration 81, loss = 1.09997864\n",
      "Iteration 82, loss = 1.09944724\n",
      "Iteration 83, loss = 1.09928140\n",
      "Iteration 84, loss = 1.09883200\n",
      "Iteration 85, loss = 1.09895742\n",
      "Iteration 86, loss = 1.09845643\n",
      "Iteration 87, loss = 1.09814765\n",
      "Iteration 88, loss = 1.09792000\n",
      "Iteration 89, loss = 1.09758223\n",
      "Iteration 90, loss = 1.09710942\n",
      "Iteration 91, loss = 1.09702670\n",
      "Iteration 92, loss = 1.09770120\n",
      "Iteration 93, loss = 1.09696574\n",
      "Iteration 94, loss = 1.09678423\n",
      "Iteration 95, loss = 1.09667799\n",
      "Iteration 96, loss = 1.09575257\n",
      "Iteration 97, loss = 1.09619408\n",
      "Iteration 98, loss = 1.09566217\n",
      "Iteration 99, loss = 1.09538518\n",
      "Iteration 100, loss = 1.09599634\n",
      "Iteration 101, loss = 1.09535542\n",
      "Iteration 102, loss = 1.09560590\n",
      "Iteration 103, loss = 1.09456815\n",
      "Iteration 104, loss = 1.09480984\n",
      "Iteration 105, loss = 1.09488888\n",
      "Iteration 106, loss = 1.09420295\n",
      "Iteration 107, loss = 1.09420224\n",
      "Iteration 108, loss = 1.09391500\n",
      "Iteration 109, loss = 1.09366069\n",
      "Iteration 110, loss = 1.09362659\n",
      "Iteration 111, loss = 1.09322085\n",
      "Iteration 112, loss = 1.09333044\n",
      "Iteration 113, loss = 1.09336009\n",
      "Iteration 114, loss = 1.09279050\n",
      "Iteration 115, loss = 1.09340101\n",
      "Iteration 116, loss = 1.09330203\n",
      "Iteration 117, loss = 1.09300482\n",
      "Iteration 118, loss = 1.09251921\n",
      "Iteration 119, loss = 1.09272922\n",
      "Iteration 120, loss = 1.09244832\n",
      "Iteration 121, loss = 1.09183747\n",
      "Iteration 122, loss = 1.09148393\n",
      "Iteration 123, loss = 1.09211267\n",
      "Iteration 124, loss = 1.09193846\n",
      "Iteration 125, loss = 1.09126070\n",
      "Iteration 126, loss = 1.09122074\n",
      "Iteration 127, loss = 1.09127015\n",
      "Iteration 128, loss = 1.09122520\n",
      "Iteration 129, loss = 1.09095032\n",
      "Iteration 130, loss = 1.09016772\n",
      "Iteration 131, loss = 1.09078968\n",
      "Iteration 132, loss = 1.09088930\n",
      "Iteration 133, loss = 1.09054844\n",
      "Iteration 134, loss = 1.09003602\n",
      "Iteration 135, loss = 1.09017050\n",
      "Iteration 136, loss = 1.08996263\n",
      "Iteration 137, loss = 1.09013388\n",
      "Iteration 138, loss = 1.09011621\n",
      "Iteration 139, loss = 1.08916763\n",
      "Iteration 140, loss = 1.08927499\n",
      "Iteration 141, loss = 1.08939965\n",
      "Iteration 142, loss = 1.08936685\n",
      "Iteration 143, loss = 1.08906816\n",
      "Iteration 144, loss = 1.08897809\n",
      "Iteration 145, loss = 1.08868408\n",
      "Iteration 146, loss = 1.08899173\n",
      "Iteration 147, loss = 1.08890718\n",
      "Iteration 148, loss = 1.08874383\n",
      "Iteration 149, loss = 1.08815240\n",
      "Iteration 150, loss = 1.08800844\n",
      "Iteration 151, loss = 1.08797402\n",
      "Iteration 152, loss = 1.08798100\n",
      "Iteration 153, loss = 1.08785541\n",
      "Iteration 154, loss = 1.08755665\n",
      "Iteration 155, loss = 1.08754730\n",
      "Iteration 156, loss = 1.08748899\n",
      "Iteration 157, loss = 1.08738508\n",
      "Iteration 158, loss = 1.08723628\n",
      "Iteration 159, loss = 1.08766772\n",
      "Iteration 160, loss = 1.08729591\n",
      "Iteration 161, loss = 1.08672969\n",
      "Iteration 162, loss = 1.08654989\n",
      "Iteration 163, loss = 1.08696500\n",
      "Iteration 164, loss = 1.08748344\n",
      "Iteration 165, loss = 1.08655629\n",
      "Iteration 166, loss = 1.08613829\n",
      "Iteration 167, loss = 1.08632073\n",
      "Iteration 168, loss = 1.08690297\n",
      "Iteration 169, loss = 1.08624313\n",
      "Iteration 170, loss = 1.08582775\n",
      "Iteration 171, loss = 1.08664798\n",
      "Iteration 172, loss = 1.08581110\n",
      "Iteration 173, loss = 1.08549217\n",
      "Iteration 174, loss = 1.08572167\n",
      "Iteration 175, loss = 1.08551728\n",
      "Iteration 176, loss = 1.08562511\n",
      "Iteration 177, loss = 1.08548488\n",
      "Iteration 178, loss = 1.08535913\n",
      "Iteration 179, loss = 1.08540672\n",
      "Iteration 180, loss = 1.08533103\n",
      "Iteration 181, loss = 1.08485680\n",
      "Iteration 182, loss = 1.08491082\n",
      "Iteration 183, loss = 1.08522183\n",
      "Iteration 184, loss = 1.08486282\n",
      "Iteration 185, loss = 1.08530172\n",
      "Iteration 186, loss = 1.08447028\n",
      "Iteration 187, loss = 1.08505331\n",
      "Iteration 188, loss = 1.08463666\n",
      "Iteration 189, loss = 1.08415938\n",
      "Iteration 190, loss = 1.08459190\n",
      "Iteration 191, loss = 1.08467597\n",
      "Iteration 192, loss = 1.08408123\n",
      "Iteration 193, loss = 1.08450501\n",
      "Iteration 194, loss = 1.08371034\n",
      "Iteration 195, loss = 1.08434703\n",
      "Iteration 196, loss = 1.08385885\n",
      "Iteration 197, loss = 1.08354179\n",
      "Iteration 198, loss = 1.08380489\n",
      "Iteration 199, loss = 1.08377426\n",
      "Iteration 200, loss = 1.08366553\n",
      "Iteration 1, loss = 1.20125769\n",
      "Iteration 2, loss = 1.18332049\n",
      "Iteration 3, loss = 1.17802985\n",
      "Iteration 4, loss = 1.17290524\n",
      "Iteration 5, loss = 1.16878637\n",
      "Iteration 6, loss = 1.16540594\n",
      "Iteration 7, loss = 1.16245229\n",
      "Iteration 8, loss = 1.15956201\n",
      "Iteration 9, loss = 1.15710892\n",
      "Iteration 10, loss = 1.15397803\n",
      "Iteration 11, loss = 1.15151355\n",
      "Iteration 12, loss = 1.14990503\n",
      "Iteration 13, loss = 1.14747538\n",
      "Iteration 14, loss = 1.14548561\n",
      "Iteration 15, loss = 1.14372790\n",
      "Iteration 16, loss = 1.14218560\n",
      "Iteration 17, loss = 1.14017731\n",
      "Iteration 18, loss = 1.13902503\n",
      "Iteration 19, loss = 1.13744003\n",
      "Iteration 20, loss = 1.13632812\n",
      "Iteration 21, loss = 1.13494464\n",
      "Iteration 22, loss = 1.13343370\n",
      "Iteration 23, loss = 1.13248345\n",
      "Iteration 24, loss = 1.13154004\n",
      "Iteration 25, loss = 1.13008262\n",
      "Iteration 26, loss = 1.12899094\n",
      "Iteration 27, loss = 1.12813617\n",
      "Iteration 28, loss = 1.12759791\n",
      "Iteration 29, loss = 1.12642222\n",
      "Iteration 30, loss = 1.12580978\n",
      "Iteration 31, loss = 1.12436745\n",
      "Iteration 32, loss = 1.12380181\n",
      "Iteration 33, loss = 1.12300822\n",
      "Iteration 34, loss = 1.12238472\n",
      "Iteration 35, loss = 1.12120053\n",
      "Iteration 36, loss = 1.12048263\n",
      "Iteration 37, loss = 1.11962573\n",
      "Iteration 38, loss = 1.11900792\n",
      "Iteration 39, loss = 1.11892860\n",
      "Iteration 40, loss = 1.11774107\n",
      "Iteration 41, loss = 1.11743784\n",
      "Iteration 42, loss = 1.11662129\n",
      "Iteration 43, loss = 1.11657395\n",
      "Iteration 44, loss = 1.11553356\n",
      "Iteration 45, loss = 1.11500136\n",
      "Iteration 46, loss = 1.11417596\n",
      "Iteration 47, loss = 1.11407413\n",
      "Iteration 48, loss = 1.11296875\n",
      "Iteration 49, loss = 1.11237610\n",
      "Iteration 50, loss = 1.11176283\n",
      "Iteration 51, loss = 1.11136403\n",
      "Iteration 52, loss = 1.11106434\n",
      "Iteration 53, loss = 1.11062422\n",
      "Iteration 54, loss = 1.10998140\n",
      "Iteration 55, loss = 1.10945989\n",
      "Iteration 56, loss = 1.10898964\n",
      "Iteration 57, loss = 1.10815692\n",
      "Iteration 58, loss = 1.10836119\n",
      "Iteration 59, loss = 1.10850981\n",
      "Iteration 60, loss = 1.10805994\n",
      "Iteration 61, loss = 1.10719190\n",
      "Iteration 62, loss = 1.10716600\n",
      "Iteration 63, loss = 1.10614727\n",
      "Iteration 64, loss = 1.10619481\n",
      "Iteration 65, loss = 1.10566356\n",
      "Iteration 66, loss = 1.10555483\n",
      "Iteration 67, loss = 1.10519221\n",
      "Iteration 68, loss = 1.10502138\n",
      "Iteration 69, loss = 1.10442064\n",
      "Iteration 70, loss = 1.10415106\n",
      "Iteration 71, loss = 1.10307835\n",
      "Iteration 72, loss = 1.10323223\n",
      "Iteration 73, loss = 1.10281854\n",
      "Iteration 74, loss = 1.10257656\n",
      "Iteration 75, loss = 1.10212988\n",
      "Iteration 76, loss = 1.10212860\n",
      "Iteration 77, loss = 1.10198529\n",
      "Iteration 78, loss = 1.10166429\n",
      "Iteration 79, loss = 1.10126126\n",
      "Iteration 80, loss = 1.10107098\n",
      "Iteration 81, loss = 1.10051670\n",
      "Iteration 82, loss = 1.10033402\n",
      "Iteration 83, loss = 1.09985417\n",
      "Iteration 84, loss = 1.09995137\n",
      "Iteration 85, loss = 1.09976785\n",
      "Iteration 86, loss = 1.09916880\n",
      "Iteration 87, loss = 1.09952168\n",
      "Iteration 88, loss = 1.09914133\n",
      "Iteration 89, loss = 1.09830775\n",
      "Iteration 90, loss = 1.09823102\n",
      "Iteration 91, loss = 1.09812109\n",
      "Iteration 92, loss = 1.09780254\n",
      "Iteration 93, loss = 1.09754330\n",
      "Iteration 94, loss = 1.09714117\n",
      "Iteration 95, loss = 1.09746412\n",
      "Iteration 96, loss = 1.09690430\n",
      "Iteration 97, loss = 1.09709392\n",
      "Iteration 98, loss = 1.09660596\n",
      "Iteration 99, loss = 1.09674148\n",
      "Iteration 100, loss = 1.09609188\n",
      "Iteration 101, loss = 1.09553407\n",
      "Iteration 102, loss = 1.09599082\n",
      "Iteration 103, loss = 1.09515939\n",
      "Iteration 104, loss = 1.09567588\n",
      "Iteration 105, loss = 1.09473434\n",
      "Iteration 106, loss = 1.09528183\n",
      "Iteration 107, loss = 1.09537888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 108, loss = 1.09434948\n",
      "Iteration 109, loss = 1.09450765\n",
      "Iteration 110, loss = 1.09442066\n",
      "Iteration 111, loss = 1.09395750\n",
      "Iteration 112, loss = 1.09334317\n",
      "Iteration 113, loss = 1.09375128\n",
      "Iteration 114, loss = 1.09359828\n",
      "Iteration 115, loss = 1.09357421\n",
      "Iteration 116, loss = 1.09339230\n",
      "Iteration 117, loss = 1.09312279\n",
      "Iteration 118, loss = 1.09290808\n",
      "Iteration 119, loss = 1.09288474\n",
      "Iteration 120, loss = 1.09248419\n",
      "Iteration 121, loss = 1.09187493\n",
      "Iteration 122, loss = 1.09254215\n",
      "Iteration 123, loss = 1.09195430\n",
      "Iteration 124, loss = 1.09227836\n",
      "Iteration 125, loss = 1.09197253\n",
      "Iteration 126, loss = 1.09133517\n",
      "Iteration 127, loss = 1.09149376\n",
      "Iteration 128, loss = 1.09125659\n",
      "Iteration 129, loss = 1.09153873\n",
      "Iteration 130, loss = 1.09094886\n",
      "Iteration 131, loss = 1.09092735\n",
      "Iteration 132, loss = 1.09061190\n",
      "Iteration 133, loss = 1.09047086\n",
      "Iteration 134, loss = 1.09017974\n",
      "Iteration 135, loss = 1.09082538\n",
      "Iteration 136, loss = 1.08976676\n",
      "Iteration 137, loss = 1.09020149\n",
      "Iteration 138, loss = 1.08983996\n",
      "Iteration 139, loss = 1.08979973\n",
      "Iteration 140, loss = 1.09023769\n",
      "Iteration 141, loss = 1.08941367\n",
      "Iteration 142, loss = 1.08906755\n",
      "Iteration 143, loss = 1.08933083\n",
      "Iteration 144, loss = 1.08902400\n",
      "Iteration 145, loss = 1.08868316\n",
      "Iteration 146, loss = 1.08890850\n",
      "Iteration 147, loss = 1.08898062\n",
      "Iteration 148, loss = 1.08843814\n",
      "Iteration 149, loss = 1.08895501\n",
      "Iteration 150, loss = 1.08842204\n",
      "Iteration 151, loss = 1.08819000\n",
      "Iteration 152, loss = 1.08796230\n",
      "Iteration 153, loss = 1.08793050\n",
      "Iteration 154, loss = 1.08793915\n",
      "Iteration 155, loss = 1.08778866\n",
      "Iteration 156, loss = 1.08823060\n",
      "Iteration 157, loss = 1.08741756\n",
      "Iteration 158, loss = 1.08701655\n",
      "Iteration 159, loss = 1.08704683\n",
      "Iteration 160, loss = 1.08698567\n",
      "Iteration 161, loss = 1.08782795\n",
      "Iteration 162, loss = 1.08677822\n",
      "Iteration 163, loss = 1.08688084\n",
      "Iteration 164, loss = 1.08662570\n",
      "Iteration 165, loss = 1.08699142\n",
      "Iteration 166, loss = 1.08739728\n",
      "Iteration 167, loss = 1.08643620\n",
      "Iteration 168, loss = 1.08636726\n",
      "Iteration 169, loss = 1.08672516\n",
      "Iteration 170, loss = 1.08607701\n",
      "Iteration 171, loss = 1.08603016\n",
      "Iteration 172, loss = 1.08628849\n",
      "Iteration 173, loss = 1.08623083\n",
      "Iteration 174, loss = 1.08593009\n",
      "Iteration 175, loss = 1.08574464\n",
      "Iteration 176, loss = 1.08584945\n",
      "Iteration 177, loss = 1.08584025\n",
      "Iteration 178, loss = 1.08537742\n",
      "Iteration 179, loss = 1.08540093\n",
      "Iteration 180, loss = 1.08530957\n",
      "Iteration 181, loss = 1.08549373\n",
      "Iteration 182, loss = 1.08563131\n",
      "Iteration 183, loss = 1.08527582\n",
      "Iteration 184, loss = 1.08473728\n",
      "Iteration 185, loss = 1.08481770\n",
      "Iteration 186, loss = 1.08491857\n",
      "Iteration 187, loss = 1.08484512\n",
      "Iteration 188, loss = 1.08467350\n",
      "Iteration 189, loss = 1.08421696\n",
      "Iteration 190, loss = 1.08432140\n",
      "Iteration 191, loss = 1.08470340\n",
      "Iteration 192, loss = 1.08422301\n",
      "Iteration 193, loss = 1.08440413\n",
      "Iteration 194, loss = 1.08448950\n",
      "Iteration 195, loss = 1.08463100\n",
      "Iteration 196, loss = 1.08442256\n",
      "Iteration 197, loss = 1.08423052\n",
      "Iteration 198, loss = 1.08382648\n",
      "Iteration 199, loss = 1.08380067\n",
      "Iteration 200, loss = 1.08390648\n",
      "Iteration 1, loss = 1.20017137\n",
      "Iteration 2, loss = 1.18381854\n",
      "Iteration 3, loss = 1.17767523\n",
      "Iteration 4, loss = 1.17256932\n",
      "Iteration 5, loss = 1.16865091\n",
      "Iteration 6, loss = 1.16438534\n",
      "Iteration 7, loss = 1.16037020\n",
      "Iteration 8, loss = 1.15736091\n",
      "Iteration 9, loss = 1.15394134\n",
      "Iteration 10, loss = 1.15179540\n",
      "Iteration 11, loss = 1.14894325\n",
      "Iteration 12, loss = 1.14668831\n",
      "Iteration 13, loss = 1.14494149\n",
      "Iteration 14, loss = 1.14198839\n",
      "Iteration 15, loss = 1.14011576\n",
      "Iteration 16, loss = 1.13860846\n",
      "Iteration 17, loss = 1.13654145\n",
      "Iteration 18, loss = 1.13523526\n",
      "Iteration 19, loss = 1.13363844\n",
      "Iteration 20, loss = 1.13252189\n",
      "Iteration 21, loss = 1.13104183\n",
      "Iteration 22, loss = 1.12911745\n",
      "Iteration 23, loss = 1.12795086\n",
      "Iteration 24, loss = 1.12737323\n",
      "Iteration 25, loss = 1.12559066\n",
      "Iteration 26, loss = 1.12430565\n",
      "Iteration 27, loss = 1.12302003\n",
      "Iteration 28, loss = 1.12241013\n",
      "Iteration 29, loss = 1.12125956\n",
      "Iteration 30, loss = 1.12006597\n",
      "Iteration 31, loss = 1.11962006\n",
      "Iteration 32, loss = 1.11861172\n",
      "Iteration 33, loss = 1.11805174\n",
      "Iteration 34, loss = 1.11702854\n",
      "Iteration 35, loss = 1.11659472\n",
      "Iteration 36, loss = 1.11543501\n",
      "Iteration 37, loss = 1.11487503\n",
      "Iteration 38, loss = 1.11394069\n",
      "Iteration 39, loss = 1.11374395\n",
      "Iteration 40, loss = 1.11264877\n",
      "Iteration 41, loss = 1.11230895\n",
      "Iteration 42, loss = 1.11178987\n",
      "Iteration 43, loss = 1.11113531\n",
      "Iteration 44, loss = 1.11060890\n",
      "Iteration 45, loss = 1.11009871\n",
      "Iteration 46, loss = 1.10938208\n",
      "Iteration 47, loss = 1.10897510\n",
      "Iteration 48, loss = 1.10810609\n",
      "Iteration 49, loss = 1.10808622\n",
      "Iteration 50, loss = 1.10768299\n",
      "Iteration 51, loss = 1.10679751\n",
      "Iteration 52, loss = 1.10588317\n",
      "Iteration 53, loss = 1.10592690\n",
      "Iteration 54, loss = 1.10609283\n",
      "Iteration 55, loss = 1.10514393\n",
      "Iteration 56, loss = 1.10525458\n",
      "Iteration 57, loss = 1.10451763\n",
      "Iteration 58, loss = 1.10436761\n",
      "Iteration 59, loss = 1.10322071\n",
      "Iteration 60, loss = 1.10299769\n",
      "Iteration 61, loss = 1.10281457\n",
      "Iteration 62, loss = 1.10270762\n",
      "Iteration 63, loss = 1.10204809\n",
      "Iteration 64, loss = 1.10223749\n",
      "Iteration 65, loss = 1.10165136\n",
      "Iteration 66, loss = 1.10177395\n",
      "Iteration 67, loss = 1.10037359\n",
      "Iteration 68, loss = 1.10057911\n",
      "Iteration 69, loss = 1.10004499\n",
      "Iteration 70, loss = 1.09983135\n",
      "Iteration 71, loss = 1.09930273\n",
      "Iteration 72, loss = 1.09965486\n",
      "Iteration 73, loss = 1.09858632\n",
      "Iteration 74, loss = 1.09858307\n",
      "Iteration 75, loss = 1.09849352\n",
      "Iteration 76, loss = 1.09810539\n",
      "Iteration 77, loss = 1.09752772\n",
      "Iteration 78, loss = 1.09766632\n",
      "Iteration 79, loss = 1.09716195\n",
      "Iteration 80, loss = 1.09645115\n",
      "Iteration 81, loss = 1.09651500\n",
      "Iteration 82, loss = 1.09669261\n",
      "Iteration 83, loss = 1.09606137\n",
      "Iteration 84, loss = 1.09578450\n",
      "Iteration 85, loss = 1.09589805\n",
      "Iteration 86, loss = 1.09525848\n",
      "Iteration 87, loss = 1.09510214\n",
      "Iteration 88, loss = 1.09474822\n",
      "Iteration 89, loss = 1.09454490\n",
      "Iteration 90, loss = 1.09466970\n",
      "Iteration 91, loss = 1.09452404\n",
      "Iteration 92, loss = 1.09417407\n",
      "Iteration 93, loss = 1.09369721\n",
      "Iteration 94, loss = 1.09341932\n",
      "Iteration 95, loss = 1.09304039\n",
      "Iteration 96, loss = 1.09281038\n",
      "Iteration 97, loss = 1.09289098\n",
      "Iteration 98, loss = 1.09274209\n",
      "Iteration 99, loss = 1.09240060\n",
      "Iteration 100, loss = 1.09210901\n",
      "Iteration 101, loss = 1.09207331\n",
      "Iteration 102, loss = 1.09190913\n",
      "Iteration 103, loss = 1.09179850\n",
      "Iteration 104, loss = 1.09149374\n",
      "Iteration 105, loss = 1.09093398\n",
      "Iteration 106, loss = 1.09098016\n",
      "Iteration 107, loss = 1.09053138\n",
      "Iteration 108, loss = 1.09066367\n",
      "Iteration 109, loss = 1.09079455\n",
      "Iteration 110, loss = 1.09107401\n",
      "Iteration 111, loss = 1.09051785\n",
      "Iteration 112, loss = 1.09002398\n",
      "Iteration 113, loss = 1.09040064\n",
      "Iteration 114, loss = 1.09026695\n",
      "Iteration 115, loss = 1.08984617\n",
      "Iteration 116, loss = 1.08975094\n",
      "Iteration 117, loss = 1.08966627\n",
      "Iteration 118, loss = 1.08902373\n",
      "Iteration 119, loss = 1.08902299\n",
      "Iteration 120, loss = 1.08900999\n",
      "Iteration 121, loss = 1.08854796\n",
      "Iteration 122, loss = 1.08846376\n",
      "Iteration 123, loss = 1.08858909\n",
      "Iteration 124, loss = 1.08866845\n",
      "Iteration 125, loss = 1.08854775\n",
      "Iteration 126, loss = 1.08809643\n",
      "Iteration 127, loss = 1.08763130\n",
      "Iteration 128, loss = 1.08814022\n",
      "Iteration 129, loss = 1.08785436\n",
      "Iteration 130, loss = 1.08805337\n",
      "Iteration 131, loss = 1.08751804\n",
      "Iteration 132, loss = 1.08737154\n",
      "Iteration 133, loss = 1.08702431\n",
      "Iteration 134, loss = 1.08738476\n",
      "Iteration 135, loss = 1.08727216\n",
      "Iteration 136, loss = 1.08700757\n",
      "Iteration 137, loss = 1.08672311\n",
      "Iteration 138, loss = 1.08680301\n",
      "Iteration 139, loss = 1.08678050\n",
      "Iteration 140, loss = 1.08693775\n",
      "Iteration 141, loss = 1.08675154\n",
      "Iteration 142, loss = 1.08674815\n",
      "Iteration 143, loss = 1.08598809\n",
      "Iteration 144, loss = 1.08617371\n",
      "Iteration 145, loss = 1.08576746\n",
      "Iteration 146, loss = 1.08600014\n",
      "Iteration 147, loss = 1.08575093\n",
      "Iteration 148, loss = 1.08603589\n",
      "Iteration 149, loss = 1.08527366\n",
      "Iteration 150, loss = 1.08568062\n",
      "Iteration 151, loss = 1.08546603\n",
      "Iteration 152, loss = 1.08499443\n",
      "Iteration 153, loss = 1.08469808\n",
      "Iteration 154, loss = 1.08476514\n",
      "Iteration 155, loss = 1.08504700\n",
      "Iteration 156, loss = 1.08486134\n",
      "Iteration 157, loss = 1.08400026\n",
      "Iteration 158, loss = 1.08458882\n",
      "Iteration 159, loss = 1.08394291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 160, loss = 1.08442842\n",
      "Iteration 161, loss = 1.08414198\n",
      "Iteration 162, loss = 1.08425199\n",
      "Iteration 163, loss = 1.08447088\n",
      "Iteration 164, loss = 1.08382221\n",
      "Iteration 165, loss = 1.08344135\n",
      "Iteration 166, loss = 1.08341611\n",
      "Iteration 167, loss = 1.08374803\n",
      "Iteration 168, loss = 1.08370573\n",
      "Iteration 169, loss = 1.08324720\n",
      "Iteration 170, loss = 1.08319691\n",
      "Iteration 171, loss = 1.08296200\n",
      "Iteration 172, loss = 1.08273188\n",
      "Iteration 173, loss = 1.08333068\n",
      "Iteration 174, loss = 1.08283850\n",
      "Iteration 175, loss = 1.08286520\n",
      "Iteration 176, loss = 1.08226930\n",
      "Iteration 177, loss = 1.08264010\n",
      "Iteration 178, loss = 1.08247206\n",
      "Iteration 179, loss = 1.08271013\n",
      "Iteration 180, loss = 1.08212598\n",
      "Iteration 181, loss = 1.08271919\n",
      "Iteration 182, loss = 1.08206236\n",
      "Iteration 183, loss = 1.08177360\n",
      "Iteration 184, loss = 1.08190535\n",
      "Iteration 185, loss = 1.08283624\n",
      "Iteration 186, loss = 1.08235191\n",
      "Iteration 187, loss = 1.08176956\n",
      "Iteration 188, loss = 1.08185618\n",
      "Iteration 189, loss = 1.08172913\n",
      "Iteration 190, loss = 1.08195318\n",
      "Iteration 191, loss = 1.08164367\n",
      "Iteration 192, loss = 1.08186117\n",
      "Iteration 193, loss = 1.08074505\n",
      "Iteration 194, loss = 1.08183244\n",
      "Iteration 195, loss = 1.08160884\n",
      "Iteration 196, loss = 1.08078935\n",
      "Iteration 197, loss = 1.08093229\n",
      "Iteration 198, loss = 1.08117919\n",
      "Iteration 199, loss = 1.08139482\n",
      "Iteration 200, loss = 1.08142739\n",
      "Iteration 1, loss = 1.20190500\n",
      "Iteration 2, loss = 1.18519662\n",
      "Iteration 3, loss = 1.17916274\n",
      "Iteration 4, loss = 1.17446843\n",
      "Iteration 5, loss = 1.17011989\n",
      "Iteration 6, loss = 1.16634104\n",
      "Iteration 7, loss = 1.16290213\n",
      "Iteration 8, loss = 1.15980368\n",
      "Iteration 9, loss = 1.15666461\n",
      "Iteration 10, loss = 1.15402452\n",
      "Iteration 11, loss = 1.15157039\n",
      "Iteration 12, loss = 1.14938801\n",
      "Iteration 13, loss = 1.14748525\n",
      "Iteration 14, loss = 1.14501586\n",
      "Iteration 15, loss = 1.14321679\n",
      "Iteration 16, loss = 1.14149756\n",
      "Iteration 17, loss = 1.13971156\n",
      "Iteration 18, loss = 1.13792295\n",
      "Iteration 19, loss = 1.13664497\n",
      "Iteration 20, loss = 1.13549950\n",
      "Iteration 21, loss = 1.13398955\n",
      "Iteration 22, loss = 1.13255172\n",
      "Iteration 23, loss = 1.13167975\n",
      "Iteration 24, loss = 1.13001551\n",
      "Iteration 25, loss = 1.12933741\n",
      "Iteration 26, loss = 1.12815988\n",
      "Iteration 27, loss = 1.12670236\n",
      "Iteration 28, loss = 1.12613350\n",
      "Iteration 29, loss = 1.12589721\n",
      "Iteration 30, loss = 1.12380965\n",
      "Iteration 31, loss = 1.12333109\n",
      "Iteration 32, loss = 1.12239064\n",
      "Iteration 33, loss = 1.12155604\n",
      "Iteration 34, loss = 1.12103969\n",
      "Iteration 35, loss = 1.12035507\n",
      "Iteration 36, loss = 1.11968526\n",
      "Iteration 37, loss = 1.11874166\n",
      "Iteration 38, loss = 1.11814921\n",
      "Iteration 39, loss = 1.11723633\n",
      "Iteration 40, loss = 1.11682081\n",
      "Iteration 41, loss = 1.11591312\n",
      "Iteration 42, loss = 1.11531211\n",
      "Iteration 43, loss = 1.11545653\n",
      "Iteration 44, loss = 1.11463698\n",
      "Iteration 45, loss = 1.11393432\n",
      "Iteration 46, loss = 1.11310832\n",
      "Iteration 47, loss = 1.11266043\n",
      "Iteration 48, loss = 1.11211456\n",
      "Iteration 49, loss = 1.11200216\n",
      "Iteration 50, loss = 1.11144934\n",
      "Iteration 51, loss = 1.11093706\n",
      "Iteration 52, loss = 1.11041292\n",
      "Iteration 53, loss = 1.10991722\n",
      "Iteration 54, loss = 1.10965407\n",
      "Iteration 55, loss = 1.10892446\n",
      "Iteration 56, loss = 1.10834146\n",
      "Iteration 57, loss = 1.10823670\n",
      "Iteration 58, loss = 1.10740254\n",
      "Iteration 59, loss = 1.10742395\n",
      "Iteration 60, loss = 1.10680099\n",
      "Iteration 61, loss = 1.10712886\n",
      "Iteration 62, loss = 1.10659174\n",
      "Iteration 63, loss = 1.10545115\n",
      "Iteration 64, loss = 1.10528748\n",
      "Iteration 65, loss = 1.10443359\n",
      "Iteration 66, loss = 1.10447538\n",
      "Iteration 67, loss = 1.10407975\n",
      "Iteration 68, loss = 1.10402942\n",
      "Iteration 69, loss = 1.10397232\n",
      "Iteration 70, loss = 1.10284747\n",
      "Iteration 71, loss = 1.10334584\n",
      "Iteration 72, loss = 1.10266462\n",
      "Iteration 73, loss = 1.10310640\n",
      "Iteration 74, loss = 1.10206260\n",
      "Iteration 75, loss = 1.10216164\n",
      "Iteration 76, loss = 1.10162780\n",
      "Iteration 77, loss = 1.10161662\n",
      "Iteration 78, loss = 1.10159431\n",
      "Iteration 79, loss = 1.10121185\n",
      "Iteration 80, loss = 1.10018004\n",
      "Iteration 81, loss = 1.10048539\n",
      "Iteration 82, loss = 1.10012282\n",
      "Iteration 83, loss = 1.09971892\n",
      "Iteration 84, loss = 1.09976058\n",
      "Iteration 85, loss = 1.09968034\n",
      "Iteration 86, loss = 1.09893227\n",
      "Iteration 87, loss = 1.09883769\n",
      "Iteration 88, loss = 1.09870935\n",
      "Iteration 89, loss = 1.09865131\n",
      "Iteration 90, loss = 1.09808721\n",
      "Iteration 91, loss = 1.09775484\n",
      "Iteration 92, loss = 1.09795344\n",
      "Iteration 93, loss = 1.09705022\n",
      "Iteration 94, loss = 1.09790738\n",
      "Iteration 95, loss = 1.09692258\n",
      "Iteration 96, loss = 1.09690483\n",
      "Iteration 97, loss = 1.09680837\n",
      "Iteration 98, loss = 1.09657482\n",
      "Iteration 99, loss = 1.09628648\n",
      "Iteration 100, loss = 1.09594791\n",
      "Iteration 101, loss = 1.09634192\n",
      "Iteration 102, loss = 1.09542471\n",
      "Iteration 103, loss = 1.09556363\n",
      "Iteration 104, loss = 1.09571535\n",
      "Iteration 105, loss = 1.09505742\n",
      "Iteration 106, loss = 1.09488893\n",
      "Iteration 107, loss = 1.09464332\n",
      "Iteration 108, loss = 1.09449894\n",
      "Iteration 109, loss = 1.09450079\n",
      "Iteration 110, loss = 1.09413484\n",
      "Iteration 111, loss = 1.09443071\n",
      "Iteration 112, loss = 1.09438304\n",
      "Iteration 113, loss = 1.09373341\n",
      "Iteration 114, loss = 1.09342488\n",
      "Iteration 115, loss = 1.09282865\n",
      "Iteration 116, loss = 1.09332337\n",
      "Iteration 117, loss = 1.09311417\n",
      "Iteration 118, loss = 1.09291274\n",
      "Iteration 119, loss = 1.09257180\n",
      "Iteration 120, loss = 1.09272746\n",
      "Iteration 121, loss = 1.09245066\n",
      "Iteration 122, loss = 1.09198628\n",
      "Iteration 123, loss = 1.09177451\n",
      "Iteration 124, loss = 1.09177912\n",
      "Iteration 125, loss = 1.09174019\n",
      "Iteration 126, loss = 1.09147708\n",
      "Iteration 127, loss = 1.09107228\n",
      "Iteration 128, loss = 1.09143547\n",
      "Iteration 129, loss = 1.09085559\n",
      "Iteration 130, loss = 1.09070089\n",
      "Iteration 131, loss = 1.09074511\n",
      "Iteration 132, loss = 1.09052250\n",
      "Iteration 133, loss = 1.09105977\n",
      "Iteration 134, loss = 1.09061817\n",
      "Iteration 135, loss = 1.09028134\n",
      "Iteration 136, loss = 1.09007217\n",
      "Iteration 137, loss = 1.09009443\n",
      "Iteration 138, loss = 1.08915364\n",
      "Iteration 139, loss = 1.08962742\n",
      "Iteration 140, loss = 1.09017762\n",
      "Iteration 141, loss = 1.08895104\n",
      "Iteration 142, loss = 1.08951461\n",
      "Iteration 143, loss = 1.08855141\n",
      "Iteration 144, loss = 1.08909691\n",
      "Iteration 145, loss = 1.08901719\n",
      "Iteration 146, loss = 1.08847766\n",
      "Iteration 147, loss = 1.08824509\n",
      "Iteration 148, loss = 1.08829100\n",
      "Iteration 149, loss = 1.08851085\n",
      "Iteration 150, loss = 1.08847563\n",
      "Iteration 151, loss = 1.08853658\n",
      "Iteration 152, loss = 1.08801790\n",
      "Iteration 153, loss = 1.08764019\n",
      "Iteration 154, loss = 1.08793248\n",
      "Iteration 155, loss = 1.08750978\n",
      "Iteration 156, loss = 1.08797080\n",
      "Iteration 157, loss = 1.08737577\n",
      "Iteration 158, loss = 1.08731660\n",
      "Iteration 159, loss = 1.08725799\n",
      "Iteration 160, loss = 1.08677099\n",
      "Iteration 161, loss = 1.08663021\n",
      "Iteration 162, loss = 1.08659029\n",
      "Iteration 163, loss = 1.08720785\n",
      "Iteration 164, loss = 1.08668811\n",
      "Iteration 165, loss = 1.08671717\n",
      "Iteration 166, loss = 1.08683401\n",
      "Iteration 167, loss = 1.08627449\n",
      "Iteration 168, loss = 1.08701348\n",
      "Iteration 169, loss = 1.08659033\n",
      "Iteration 170, loss = 1.08608458\n",
      "Iteration 171, loss = 1.08604647\n",
      "Iteration 172, loss = 1.08613088\n",
      "Iteration 173, loss = 1.08547153\n",
      "Iteration 174, loss = 1.08579848\n",
      "Iteration 175, loss = 1.08539918\n",
      "Iteration 176, loss = 1.08542706\n",
      "Iteration 177, loss = 1.08530847\n",
      "Iteration 178, loss = 1.08536851\n",
      "Iteration 179, loss = 1.08529181\n",
      "Iteration 180, loss = 1.08525504\n",
      "Iteration 181, loss = 1.08538658\n",
      "Iteration 182, loss = 1.08500693\n",
      "Iteration 183, loss = 1.08518957\n",
      "Iteration 184, loss = 1.08527997\n",
      "Iteration 185, loss = 1.08503296\n",
      "Iteration 186, loss = 1.08401057\n",
      "Iteration 187, loss = 1.08450326\n",
      "Iteration 188, loss = 1.08438853\n",
      "Iteration 189, loss = 1.08420383\n",
      "Iteration 190, loss = 1.08455113\n",
      "Iteration 191, loss = 1.08421293\n",
      "Iteration 192, loss = 1.08463161\n",
      "Iteration 193, loss = 1.08386608\n",
      "Iteration 194, loss = 1.08422961\n",
      "Iteration 195, loss = 1.08449716\n",
      "Iteration 196, loss = 1.08371814\n",
      "Iteration 197, loss = 1.08392783\n",
      "Iteration 198, loss = 1.08358570\n",
      "Iteration 199, loss = 1.08380056\n",
      "Iteration 200, loss = 1.08298705\n",
      "corss-val acc: 0.4452109570611757\n",
      "Iteration 1, loss = 1.19858141\n",
      "Iteration 2, loss = 1.18435528\n",
      "Iteration 3, loss = 1.17843952\n",
      "Iteration 4, loss = 1.17398527\n",
      "Iteration 5, loss = 1.17004174\n",
      "Iteration 6, loss = 1.16659240\n",
      "Iteration 7, loss = 1.16388987\n",
      "Iteration 8, loss = 1.16096549\n",
      "Iteration 9, loss = 1.15857343\n",
      "Iteration 10, loss = 1.15581115\n",
      "Iteration 11, loss = 1.15378257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 1.15199900\n",
      "Iteration 13, loss = 1.15010145\n",
      "Iteration 14, loss = 1.14843112\n",
      "Iteration 15, loss = 1.14718730\n",
      "Iteration 16, loss = 1.14504910\n",
      "Iteration 17, loss = 1.14444894\n",
      "Iteration 18, loss = 1.14308488\n",
      "Iteration 19, loss = 1.14156112\n",
      "Iteration 20, loss = 1.14043066\n",
      "Iteration 21, loss = 1.13906148\n",
      "Iteration 22, loss = 1.13818002\n",
      "Iteration 23, loss = 1.13691099\n",
      "Iteration 24, loss = 1.13632888\n",
      "Iteration 25, loss = 1.13540566\n",
      "Iteration 26, loss = 1.13444101\n",
      "Iteration 27, loss = 1.13365011\n",
      "Iteration 28, loss = 1.13280592\n",
      "Iteration 29, loss = 1.13189787\n",
      "Iteration 30, loss = 1.13086470\n",
      "Iteration 31, loss = 1.13039021\n",
      "Iteration 32, loss = 1.12933600\n",
      "Iteration 33, loss = 1.12905151\n",
      "Iteration 34, loss = 1.12799712\n",
      "Iteration 35, loss = 1.12749430\n",
      "Iteration 36, loss = 1.12648558\n",
      "Iteration 37, loss = 1.12652385\n",
      "Iteration 38, loss = 1.12576506\n",
      "Iteration 39, loss = 1.12476773\n",
      "Iteration 40, loss = 1.12498810\n",
      "Iteration 41, loss = 1.12446582\n",
      "Iteration 42, loss = 1.12405212\n",
      "Iteration 43, loss = 1.12327212\n",
      "Iteration 44, loss = 1.12268327\n",
      "Iteration 45, loss = 1.12242920\n",
      "Iteration 46, loss = 1.12178835\n",
      "Iteration 47, loss = 1.12173325\n",
      "Iteration 48, loss = 1.12096833\n",
      "Iteration 49, loss = 1.12040139\n",
      "Iteration 50, loss = 1.12035388\n",
      "Iteration 51, loss = 1.11988041\n",
      "Iteration 52, loss = 1.11945091\n",
      "Iteration 53, loss = 1.11939206\n",
      "Iteration 54, loss = 1.11867577\n",
      "Iteration 55, loss = 1.11808763\n",
      "Iteration 56, loss = 1.11817513\n",
      "Iteration 57, loss = 1.11781276\n",
      "Iteration 58, loss = 1.11713339\n",
      "Iteration 59, loss = 1.11759693\n",
      "Iteration 60, loss = 1.11699946\n",
      "Iteration 61, loss = 1.11652207\n",
      "Iteration 62, loss = 1.11599476\n",
      "Iteration 63, loss = 1.11645743\n",
      "Iteration 64, loss = 1.11512886\n",
      "Iteration 65, loss = 1.11528103\n",
      "Iteration 66, loss = 1.11504699\n",
      "Iteration 67, loss = 1.11516875\n",
      "Iteration 68, loss = 1.11477353\n",
      "Iteration 69, loss = 1.11406992\n",
      "Iteration 70, loss = 1.11400842\n",
      "Iteration 71, loss = 1.11365418\n",
      "Iteration 72, loss = 1.11363601\n",
      "Iteration 73, loss = 1.11359921\n",
      "Iteration 74, loss = 1.11333244\n",
      "Iteration 75, loss = 1.11278636\n",
      "Iteration 76, loss = 1.11263164\n",
      "Iteration 77, loss = 1.11235259\n",
      "Iteration 78, loss = 1.11166837\n",
      "Iteration 79, loss = 1.11181855\n",
      "Iteration 80, loss = 1.11172169\n",
      "Iteration 81, loss = 1.11203169\n",
      "Iteration 82, loss = 1.11126597\n",
      "Iteration 83, loss = 1.11092248\n",
      "Iteration 84, loss = 1.11087015\n",
      "Iteration 85, loss = 1.11087847\n",
      "Iteration 86, loss = 1.11042726\n",
      "Iteration 87, loss = 1.10987500\n",
      "Iteration 88, loss = 1.11009159\n",
      "Iteration 89, loss = 1.10990647\n",
      "Iteration 90, loss = 1.10999472\n",
      "Iteration 91, loss = 1.10936795\n",
      "Iteration 92, loss = 1.10923845\n",
      "Iteration 93, loss = 1.10931663\n",
      "Iteration 94, loss = 1.10922899\n",
      "Iteration 95, loss = 1.10901879\n",
      "Iteration 96, loss = 1.10881491\n",
      "Iteration 97, loss = 1.10884066\n",
      "Iteration 98, loss = 1.10857492\n",
      "Iteration 99, loss = 1.10818102\n",
      "Iteration 100, loss = 1.10824662\n",
      "Iteration 101, loss = 1.10837616\n",
      "Iteration 102, loss = 1.10771742\n",
      "Iteration 103, loss = 1.10757343\n",
      "Iteration 104, loss = 1.10758227\n",
      "Iteration 105, loss = 1.10739385\n",
      "Iteration 106, loss = 1.10767536\n",
      "Iteration 107, loss = 1.10717795\n",
      "Iteration 108, loss = 1.10684933\n",
      "Iteration 109, loss = 1.10662162\n",
      "Iteration 110, loss = 1.10672064\n",
      "Iteration 111, loss = 1.10701808\n",
      "Iteration 112, loss = 1.10614388\n",
      "Iteration 113, loss = 1.10649099\n",
      "Iteration 114, loss = 1.10631665\n",
      "Iteration 115, loss = 1.10618052\n",
      "Iteration 116, loss = 1.10627710\n",
      "Iteration 117, loss = 1.10575498\n",
      "Iteration 118, loss = 1.10565556\n",
      "Iteration 119, loss = 1.10564600\n",
      "Iteration 120, loss = 1.10547035\n",
      "Iteration 121, loss = 1.10554956\n",
      "Iteration 122, loss = 1.10521348\n",
      "Iteration 123, loss = 1.10528144\n",
      "Iteration 124, loss = 1.10489764\n",
      "Iteration 125, loss = 1.10504763\n",
      "Iteration 126, loss = 1.10480660\n",
      "Iteration 127, loss = 1.10515772\n",
      "Iteration 128, loss = 1.10500095\n",
      "Iteration 129, loss = 1.10434994\n",
      "Iteration 130, loss = 1.10426053\n",
      "Iteration 131, loss = 1.10461955\n",
      "Iteration 132, loss = 1.10399426\n",
      "Iteration 133, loss = 1.10418171\n",
      "Iteration 134, loss = 1.10399257\n",
      "Iteration 135, loss = 1.10427560\n",
      "Iteration 136, loss = 1.10384233\n",
      "Iteration 137, loss = 1.10354221\n",
      "Iteration 138, loss = 1.10393527\n",
      "Iteration 139, loss = 1.10356678\n",
      "Iteration 140, loss = 1.10360056\n",
      "Iteration 141, loss = 1.10331800\n",
      "Iteration 142, loss = 1.10312520\n",
      "Iteration 143, loss = 1.10321593\n",
      "Iteration 144, loss = 1.10321072\n",
      "Iteration 145, loss = 1.10302430\n",
      "Iteration 146, loss = 1.10300978\n",
      "Iteration 147, loss = 1.10285844\n",
      "Iteration 148, loss = 1.10279703\n",
      "Iteration 149, loss = 1.10294760\n",
      "Iteration 150, loss = 1.10242582\n",
      "Iteration 151, loss = 1.10264447\n",
      "Iteration 152, loss = 1.10238694\n",
      "Iteration 153, loss = 1.10265115\n",
      "Iteration 154, loss = 1.10223988\n",
      "Iteration 155, loss = 1.10215014\n",
      "Iteration 156, loss = 1.10221628\n",
      "Iteration 157, loss = 1.10222225\n",
      "Iteration 158, loss = 1.10231462\n",
      "Iteration 159, loss = 1.10174195\n",
      "Iteration 160, loss = 1.10154572\n",
      "Iteration 161, loss = 1.10167100\n",
      "Iteration 162, loss = 1.10138561\n",
      "Iteration 163, loss = 1.10137815\n",
      "Iteration 164, loss = 1.10145916\n",
      "Iteration 165, loss = 1.10107748\n",
      "Iteration 166, loss = 1.10169406\n",
      "Iteration 167, loss = 1.10112326\n",
      "Iteration 168, loss = 1.10119003\n",
      "Iteration 169, loss = 1.10099023\n",
      "Iteration 170, loss = 1.10102195\n",
      "Iteration 171, loss = 1.10057904\n",
      "Iteration 172, loss = 1.10115554\n",
      "Iteration 173, loss = 1.10115265\n",
      "Iteration 174, loss = 1.10081894\n",
      "Iteration 175, loss = 1.10124772\n",
      "Iteration 176, loss = 1.10049089\n",
      "Iteration 177, loss = 1.10077193\n",
      "Iteration 178, loss = 1.10037863\n",
      "Iteration 179, loss = 1.10099211\n",
      "Iteration 180, loss = 1.10015479\n",
      "Iteration 181, loss = 1.10057053\n",
      "Iteration 182, loss = 1.10041995\n",
      "Iteration 183, loss = 1.09979464\n",
      "Iteration 184, loss = 1.09983537\n",
      "Iteration 185, loss = 1.10042337\n",
      "Iteration 186, loss = 1.10020699\n",
      "Iteration 187, loss = 1.09962618\n",
      "Iteration 188, loss = 1.09989673\n",
      "Iteration 189, loss = 1.09938122\n",
      "Iteration 190, loss = 1.09953393\n",
      "Iteration 191, loss = 1.09979739\n",
      "Iteration 192, loss = 1.09935308\n",
      "Iteration 193, loss = 1.09943323\n",
      "Iteration 194, loss = 1.09966330\n",
      "Iteration 195, loss = 1.09944641\n",
      "Iteration 196, loss = 1.09926750\n",
      "Iteration 197, loss = 1.09928702\n",
      "Iteration 198, loss = 1.09910965\n",
      "Iteration 199, loss = 1.09939089\n",
      "Iteration 200, loss = 1.09917092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(verbose=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = MLPClassifier(max_iter=200, verbose=True)\n",
    "\n",
    "print('corss-val acc:', np.mean(cross_val_score(per_clf, x_train, y_train, cv=5)))\n",
    "per_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_predictions = per_clf.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTrees \t\tAccuracy: 0.45\tMacro F1: 0.27\n"
     ]
    }
   ],
   "source": [
    "per_acc = accuracy_score(y_dev, per_predictions)\n",
    "per_f1 = f1_score(y_dev, per_predictions, average='macro')\n",
    "\n",
    "print(f\"ExtraTrees \\t\\tAccuracy: {round(per_acc, 2)}\\tMacro F1: {round(per_f1, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize text\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                     oov_token=\"NaN\", \n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(lst_corpus_train)\n",
    "dic_vocabulary = tokenizer.word_index \n",
    "\n",
    "## create sequence\n",
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus_train)\n",
    "\n",
    "## padding sequence\n",
    "X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
    "                    maxlen=15, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133795, 15)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from:  lol see martial art master never return anger kuhai even battle u hope learned lesson | len: 15\n",
      "to:  [    4    22  8770  2002  2509    70  1400  3825 25549    86  1867     3\n",
      "   143  1565  2378] | len: 15\n",
      "check:  lol  -- idx in vocabulary --> 4\n",
      "vocabulary:  {'NaN': 1, 'rt': 2, 'u': 3, 'lol': 4, 'im': 5} ... (padding element, 0)\n"
     ]
    }
   ],
   "source": [
    "i = 100\n",
    "\n",
    "## list of text: [\"I like this\", ...]\n",
    "len_txt = len(df_train[\"tweet_clean\"].iloc[i].split())\n",
    "print(\"from: \", df_train[\"tweet_clean\"].iloc[i], \"| len:\", len_txt)\n",
    "\n",
    "## sequence of token ids: [[1, 2, 3], ...]\n",
    "len_tokens = len(X_train[i])\n",
    "print(\"to: \", X_train[i], \"| len:\", len(X_train[i]))\n",
    "\n",
    "## vocabulary: {\"I\":1, \"like\":2, \"this\":3, ...}\n",
    "print(\"check: \", df_train[\"tweet_clean\"].iloc[i].split()[0], \n",
    "      \" -- idx in vocabulary -->\", \n",
    "      dic_vocabulary[df_train[\"tweet_clean\"].iloc[i].split()[0]])\n",
    "\n",
    "print(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:5]), \"... (padding element, 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(lst_corpus_dev)\n",
    "dic_vocabulary = tokenizer.word_index \n",
    "\n",
    "## create sequence\n",
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus_dev)\n",
    "\n",
    "## padding sequence\n",
    "X_dev = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
    "                    maxlen=15, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  w2v[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (bidirectional/forward_lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a5d493dcf174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                      input_length=15, trainable=False)(x_in)## apply attention\n\u001b[1;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m## 2 layers of bidirectional lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n\u001b[0m\u001b[1;32m     16\u001b[0m                          return_sequences=True))(x)\n\u001b[1;32m     17\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m## final dense layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    970\u001b[0m                                                 input_list)\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1106\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1108\u001b[0m           inputs, input_masks, args, kwargs)\n\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask, initial_state, constants)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mforward_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m       y = self.forward_layer(forward_inputs,\n\u001b[0m\u001b[1;32m    699\u001b[0m                              initial_state=forward_state, **kwargs)\n\u001b[1;32m    700\u001b[0m       y_rev = self.backward_layer(backward_inputs,\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;31m# LSTM does not support constants. Ignore it during process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m       init_state = get_initial_state_fn(\n\u001b[0m\u001b[1;32m    651\u001b[0m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[1;32m    652\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2515\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2516\u001b[0;31m     return list(_generate_zero_filled_state_for_cell(\n\u001b[0m\u001b[1;32m   2517\u001b[0m         self, inputs, batch_size, dtype))\n\u001b[1;32m   2518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[0;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2996\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2998\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[0;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[0;34m(unnested_state_size)\u001b[0m\n\u001b[1;32m   3009\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3011\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2911\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2912\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2958\u001b[0m           \u001b[0;31m# Create a constant if it won't be very big. Otherwise create a fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m           \u001b[0;31m# op to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2960\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2961\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[0;34m(value, shape, dtype, name)\u001b[0m\n\u001b[1;32m   2894\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2896\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mbut\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresulting\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcast\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnecessary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m     \u001b[0mReturns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m     \u001b[0mcumprod\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/iml-p3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (bidirectional/forward_lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "## code attention layer\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "## input\n",
    "x_in = layers.Input(shape=(15,))## embedding\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=15, trainable=False)(x_in)## apply attention\n",
    "x = attention_layer(x, neurons=15)## 2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                         return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)## final dense layers\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y_out = layers.Dense(3, activation='softmax')(x)## compile\n",
    "model = models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictor = lr_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test_full.csv')\n",
    "df_test = df_test[['tweet']]\n",
    "\n",
    "df_test.index = range(df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12018, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9837</th>\n",
       "      <td>Finna go to sleep.....but one note tho like my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>Better to do? I'm walking out this crazy libra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>@USER_6169045c HEY!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7062</th>\n",
       "      <td>Aye OLD drunk ladies are funny, they always wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856</th>\n",
       "      <td>RT @USER_30df73b1: its y.o.e or nuffin u figru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "9837  Finna go to sleep.....but one note tho like my...\n",
       "1707  Better to do? I'm walking out this crazy libra...\n",
       "753                               @USER_6169045c HEY!!!\n",
       "7062  Aye OLD drunk ladies are funny, they always wa...\n",
       "7856  RT @USER_30df73b1: its y.o.e or nuffin u figru..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>On my way out wit the fam!!</td>\n",
       "      <td>way wit fam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>#iBlockedYouBecause u might trick me once but ...</td>\n",
       "      <td>iblockedyoubecause u might trick wont let u tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3348</th>\n",
       "      <td>@USER_a2152876 I'm good you</td>\n",
       "      <td>im good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>@USER_37af4b80 lmao my bad...tryna get some at...</td>\n",
       "      <td>lmao badtryna get attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>@USER_81e70e70 lmfao yeaaa u right boo I'll co...</td>\n",
       "      <td>lmfao yeaaa u right boo ill come see person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "4838                        On my way out wit the fam!!   \n",
       "5761  #iBlockedYouBecause u might trick me once but ...   \n",
       "3348                        @USER_a2152876 I'm good you   \n",
       "4420  @USER_37af4b80 lmao my bad...tryna get some at...   \n",
       "5338  @USER_81e70e70 lmfao yeaaa u right boo I'll co...   \n",
       "\n",
       "                                            tweet_clean  \n",
       "4838                                        way wit fam  \n",
       "5761  iblockedyoubecause u might trick wont let u tr...  \n",
       "3348                                            im good  \n",
       "4420                        lmao badtryna get attention  \n",
       "5338        lmfao yeaaa u right boo ill come see person  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"tweet_clean\"] = df_test[\"tweet\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=False, \n",
    "          lst_stopwords=lst_stopwords))\n",
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12018, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the test data as well\n",
    "corpus_test = df_test[\"tweet_clean\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus_test = []\n",
    "for string in corpus_test:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus_test.append(lst_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12018"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors_test = transform(w2v, lst_corpus_test)\n",
    "x_test = w2v_vectors_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12018, 300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_predictor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(predictions, columns=['region'])\n",
    "result.index.name = \"id\"\n",
    "result.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "f598f9c7c2303acb1d2869a7cf251bfa8fcf007a03527c86cf2c2371d78de088"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
